{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import initialize\n",
    "from mistify import _functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_dataset1(shape, dropout_p: bool=0.5, exp: float=1.0, require_grad: bool=False):\n",
    "\n",
    "    data = torch.rand(*shape) ** exp\n",
    "    data = data * (torch.rand(shape) >= dropout_p)\n",
    "    if require_grad:\n",
    "        data.requires_grad_()\n",
    "        data.retain_grad()\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to check if the straight through estimators succeed in optimizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "def optim_op(f, x1: torch.Tensor, t, n=400, p: typing.List[torch.Tensor]=None, **kwargs):\n",
    "    x1.requires_grad_()\n",
    "    if p is None:\n",
    "        optim = torch.optim.Adam([x1], lr=1e-2)\n",
    "    else:\n",
    "        print('setting p')\n",
    "        optim = torch.optim.Adam(p, lr=1e-2)\n",
    "    for i in range(n):\n",
    "        optim.zero_grad()\n",
    "        y = f(x1, **kwargs)\n",
    "        loss = (y - t).pow(2).mean()\n",
    "        loss.backward()\n",
    "        # print('Grad: ', p[0].grad.abs().sum().item(), p[1].grad.abs().sum().item(), loss.item())\n",
    "        optim.step()\n",
    "    print('Looped : ', i, ' times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE\n",
      "HERE\n",
      "Looped :  1  times\n",
      "tensor([[0.8101, 0.4980, 0.9371, 0.6556],\n",
      "        [0.9191, 0.4367, 0.6940, 0.2876],\n",
      "        [0.9706, 0.5239, 0.8550, 0.7718],\n",
      "        [0.2566, 0.8100, 0.6397, 0.9743]], grad_fn=<MaximumBackward0>)\n",
      "tensor([[0.7576, 0.4980, 0.9371, 0.7347],\n",
      "        [0.3138, 0.7999, 0.4162, 0.7544],\n",
      "        [0.5695, 0.5239, 0.7981, 0.7718],\n",
      "        [0.6826, 0.8100, 0.6397, 0.9743]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test union\n",
    "torch.manual_seed(1)\n",
    "x1_b = torch.rand(4, 4)\n",
    "x2 = torch.rand(4, 4)\n",
    "t = F.union(x1_b, x2).detach()\n",
    "\n",
    "x1 = torch.rand(4, 4, requires_grad=True)\n",
    "optim_op(F.union, x1, t, n=500, x2=x2, g=F.ClipG(0.1))\n",
    "torch.sqrt((x1 - x1_b).pow(2).mean()).item()\n",
    "\n",
    "print(F.union(x1, x2)) \n",
    "print(F.union(x1_b, x2))\n",
    "\n",
    "# print(x1_b.detach(), x1.detach(), x2.detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting p\n",
      "Looped :  499  times\n",
      "tensor([[0.8300, 0.2793, 0.4031, 0.7347],\n",
      "        [0.9391, 0.7999, 0.7140, 0.7544],\n",
      "        [0.9906, 0.4388, 0.8750, 0.5247],\n",
      "        [0.6826, 0.7570, 0.4635, 0.6471],\n",
      "        [0.5725, 0.4980, 0.9371, 0.6556],\n",
      "        [0.7713, 0.3785, 0.9980, 0.9008],\n",
      "        [0.4766, 0.5239, 0.8045, 0.7718],\n",
      "        [0.1768, 0.8248, 0.8036, 0.9743]], grad_fn=<MaximumBackward0>)\n",
      "tensor([[0.8300, 0.2793, 0.4031, 0.7347],\n",
      "        [0.9391, 0.7999, 0.7140, 0.7544],\n",
      "        [0.9906, 0.4388, 0.8750, 0.5247],\n",
      "        [0.6826, 0.7570, 0.4635, 0.6471],\n",
      "        [0.5725, 0.4980, 0.9371, 0.6556],\n",
      "        [0.7713, 0.3785, 0.9980, 0.9008],\n",
      "        [0.4766, 0.5239, 0.8045, 0.7718],\n",
      "        [0.1768, 0.8248, 0.8036, 0.9743]])\n"
     ]
    }
   ],
   "source": [
    "# Test union 2 (both trainable)\n",
    "torch.manual_seed(1)\n",
    "x1_b = torch.rand(8, 4)\n",
    "x2_b = torch.rand(8, 4)\n",
    "t = F.union(x1_b, x2_b).detach()\n",
    "\n",
    "x1 = create_input_dataset1([8, 4], 0.75, require_grad=True)\n",
    "x2 = create_input_dataset1([8, 4], 0.75, require_grad=True)\n",
    "\n",
    "optim_op(F.union, x1, t, n=500, x2=x2, p=[x1, x2], g=F.MulG(0.1))\n",
    "torch.sqrt((x1 - x1_b).pow(2).mean()).item()\n",
    "\n",
    "print(F.union(x1, x2)) \n",
    "print(F.union(x1_b, x2_b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looped :  399  times\n",
      "tensor([[0.5725, 0.2793, 0.4031, 0.6556],\n",
      "        [0.0293, 0.1980, 0.3971, 0.2843],\n",
      "        [0.3398, 0.4388, 0.6387, 0.5247],\n",
      "        [0.0112, 0.3051, 0.4635, 0.4550]], grad_fn=<MinimumBackward0>)\n",
      "tensor([[0.5725, 0.2793, 0.4031, 0.6556],\n",
      "        [0.0293, 0.1980, 0.3971, 0.2843],\n",
      "        [0.3398, 0.4388, 0.6387, 0.5247],\n",
      "        [0.0112, 0.3051, 0.4635, 0.4550]])\n"
     ]
    }
   ],
   "source": [
    "# Test union\n",
    "torch.manual_seed(1)\n",
    "x1_b = torch.rand(4, 4)\n",
    "x2 = torch.rand(4, 4)\n",
    "t = F.inter(x1_b, x2).detach()\n",
    "\n",
    "x1 = torch.rand(4, 4, requires_grad=True)\n",
    "optim_op(F.inter, x1, t, x2=x2, g=F.ClipG(0.1))\n",
    "torch.sqrt((x1 - x1_b).pow(2).mean()).item()\n",
    "\n",
    "\n",
    "print(F.inter(x1, x2)) \n",
    "print(F.inter(x1_b, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looped :  399  times\n",
      "tensor([0.2793, 0.0293, 0.4388, 0.3051, 0.4980, 0.1980, 0.3398, 0.0112, 0.0246,\n",
      "        0.2676, 0.2885, 0.2346, 0.0193, 0.3785, 0.1663, 0.1768, 0.2197, 0.1205,\n",
      "        0.5285, 0.0369, 0.1227, 0.0555, 0.1251, 0.1652, 0.2873, 0.4075, 0.0748,\n",
      "        0.6030, 0.1682, 0.1381, 0.1842, 0.0299], grad_fn=<MinBackward0>)\n",
      "tensor([0.2793, 0.0293, 0.4388, 0.3051, 0.4980, 0.1980, 0.3398, 0.0112, 0.0246,\n",
      "        0.2676, 0.2885, 0.2346, 0.0193, 0.3785, 0.1663, 0.1768, 0.2197, 0.1205,\n",
      "        0.5285, 0.0369, 0.1227, 0.0555, 0.1251, 0.1652, 0.2873, 0.4075, 0.0748,\n",
      "        0.6036, 0.1682, 0.1381, 0.1842, 0.0299])\n"
     ]
    }
   ],
   "source": [
    "# Test inter on\n",
    "torch.manual_seed(1)\n",
    "x1_b = torch.rand(32, 4)\n",
    "# x2 = torch.rand(4, 4)\n",
    "t = F.inter_on(x1_b, -1, False).detach()\n",
    "\n",
    "x1 = torch.rand(32, 4, requires_grad=True)\n",
    "optim_op(F.inter_on, x1, t, dim=-1, keepdim=False, g=F.MulG(0.01))\n",
    "torch.sqrt((x1 - x1_b).pow(2).mean()).item()\n",
    "print(F.inter_on(x1, dim=-1)) \n",
    "print(F.inter_on(x1_b, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1168, grad_fn=<MeanBackward0>)\n",
      "Grad:  0.2834576666355133 0.09538086503744125 0.11684583872556686\n",
      "Looped :  0  times\n",
      "tensor(0.1131, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test 1 layer\n",
    "\n",
    "# This shows if i can optimize a single layer\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "g = F.MulG(0.1)\n",
    "\n",
    "def f(x1, x2):\n",
    "    return F.inter_on(F.union(x1, x2), dim=-1)\n",
    "\n",
    "x1_b = create_input_dataset1([32, 4, 1], 0.75, require_grad=True)\n",
    "x2_b = create_input_dataset1([1, 4, 8], 0.75, require_grad=True)\n",
    "t = f(x1_b, x2_b).detach()\n",
    "\n",
    "x1 = create_input_dataset1([32, 4, 1], 0.75, require_grad=True)\n",
    "x2 = create_input_dataset1([1, 4, 8], 0.75, require_grad=True)\n",
    "\n",
    "print((f(x1, x2) - f(x1_b, x2_b)).pow(2).mean())\n",
    "optim_op(f, x1, t, n=1, x2=x2, p=[x1, x2])\n",
    "torch.sqrt((x1 - x1_b).pow(2).mean()).item()\n",
    "\n",
    "print((f(x1, x2) - f(x1_b, x2_b)).pow(2).mean()) \n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0640, grad_fn=<MeanBackward0>)\n",
      "setting p\n",
      "Grad:  0.0 0.0 0.06397570669651031\n",
      "Grad:  0.0 0.0 0.06397570669651031\n",
      "Looped :  1  times\n",
      "tensor(0., grad_fn=<MeanBackward0>)\n",
      "tensor(0.0640, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test 2 layers\n",
    "\n",
    "# This shows if i can optimize a single layer\n",
    "\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(1)\n",
    "\n",
    "g = F.MulG(0.0001)\n",
    "\n",
    "def layer(x1, w, g=None):\n",
    "\n",
    "    return F.inter_on(F.union(x1, w, g=g), dim=-2, g=g)\n",
    "\n",
    "def net(x1, w1, w2):\n",
    "\n",
    "    y = layer(x1, w1)\n",
    "    y = y.unsqueeze(-1)\n",
    "    return layer(y, w2)\n",
    "\n",
    "x1_b = torch.rand(32, 32) ** 2\n",
    "\n",
    "t = nn.Linear(32, 6)(x1_b).detach()\n",
    "x1_b = x1_b.unsqueeze(-1)\n",
    "\n",
    "# x1_b = create_input_dataset1([32, 4, 1], 0, require_grad=False)\n",
    "# w1_b = create_input_dataset1([1, 32, 8], 0.5, require_grad=False)\n",
    "# w2_b = create_input_dataset1([1, 8, 6], 0.5, require_grad=False)\n",
    "\n",
    "# t = net(x1_b, w1_b, w2_b).detach()\n",
    "\n",
    "# x1 = create_input_dataset1([32, 4, 1], 0.75, require_grad=False)\n",
    "w1 = create_input_dataset1([1, 32, 8], 0.5, require_grad=True)\n",
    "w2 = create_input_dataset1([1, 8, 6], 0.5, require_grad=True)\n",
    "\n",
    "# print((w1 - w1_b).pow(2).mean())\n",
    "# print(t)\n",
    "\n",
    "print((net(x1_b, w1, w2) - t).pow(2).mean()) \n",
    "w1_clone = torch.clone(w1).detach()\n",
    "\n",
    "optim_op(net, x1_b, t, n=2, w1=w1, w2=w2, p=[w1, w2])\n",
    "# torch.sqrt((x1 - x1_b).pow(2).mean()).item()\n",
    "print((w1 - w1_clone).pow(2).mean())\n",
    "\n",
    "print((net(x1_b, w1, w2) - t).pow(2).mean()) \n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7576, 0.7999, 0.6387, 0.6826, 0.9371, 0.4162, 0.7981, 0.9743, 0.8300,\n",
      "        0.9391, 0.9906, 0.7570, 0.4452, 0.9980, 0.8045, 0.9434],\n",
      "       grad_fn=<MaxBackward0>)\n",
      "tensor([0.7576, 0.7999, 0.6387, 0.6826, 0.9371, 0.4162, 0.7981, 0.9743, 0.8300,\n",
      "        0.9391, 0.9906, 0.7570, 0.4452, 0.9980, 0.8045, 0.9434])\n"
     ]
    }
   ],
   "source": [
    "# Test union on\n",
    "torch.manual_seed(1)\n",
    "x1_b = torch.rand(16, 4)\n",
    "t = F.union_on(x1_b, -1, False).detach()\n",
    "\n",
    "x1 = torch.rand(16, 4, requires_grad=True)\n",
    "optim_op(F.union_on, x1, t, dim=-1, n=400, keepdim=False, g=F.MulG(0.01))\n",
    "# torch.sqrt((x1 - x1_b).pow(2).mean()).item()\n",
    "\n",
    "\n",
    "print(F.union_on(x1, dim=-1)) \n",
    "print(F.union_on(x1_b, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1590016782283783\n",
      "tensor([[1., 0., 1., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [0., 1., 1., 1.]], grad_fn=<BinaryGBackward>) tensor([[1., 0., 1., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [0., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Test binary\n",
    "torch.manual_seed(2)\n",
    "x1_b = torch.rand(4, 4)\n",
    "# x2 = torch.rand(4, 4)\n",
    "t = F.binarize(x1_b).detach()\n",
    "\n",
    "x1 = torch.rand(4, 4, requires_grad=True)\n",
    "optim_op(F.binarize, x1, t, n=1000, g=True, clip=0.1)\n",
    "print(torch.sqrt((x1 - x1_b).pow(2).mean()).item())\n",
    "print(F.binarize(x1), F.binarize(x1_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7361576557159424\n",
      "tensor([[-1.,  1., -1., -1.],\n",
      "        [-1.,  1., -1., -1.],\n",
      "        [-1.,  1., -1., -1.],\n",
      "        [ 1., -1., -1., -1.]], grad_fn=<SignGBackward>)\n",
      "tensor([[-1.,  1., -1., -1.],\n",
      "        [-1.,  1., -1., -1.],\n",
      "        [-1.,  1., -1., -1.],\n",
      "        [ 1., -1., -1., -1.]])\n"
     ]
    }
   ],
   "source": [
    "# Test binary\n",
    "torch.manual_seed(2)\n",
    "x1_b = torch.randn(4, 4)\n",
    "# x2 = torch.rand(4, 4)\n",
    "t = F.signify(x1_b).detach()\n",
    "\n",
    "x1 = torch.rand(4, 4, requires_grad=True)\n",
    "optim_op(F.signify, x1, t, n=1000, g=True, clip=0.1)\n",
    "print(torch.sqrt((x1 - x1_b).pow(2).mean()).item())\n",
    "print(F.signify(x1)) \n",
    "print(F.signify(x1_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3310493528842926\n",
      "tensor([[0.8441, 0.1430, 0.9113, 0.4234],\n",
      "        [1.0000, 0.8571, 0.3276, 0.0000],\n",
      "        [0.8425, 0.0000, 0.6971, 0.5997],\n",
      "        [0.1702, 1.0000, 0.6001, 1.0000]], grad_fn=<ClampGBackward>)\n",
      "tensor([[0.8441, 0.1430, 0.9113, 0.4234],\n",
      "        [1.0000, 0.8571, 0.3276, 0.0000],\n",
      "        [0.8425, 0.0000, 0.6971, 0.5997],\n",
      "        [0.1702, 1.0000, 0.6001, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Test clamp\n",
    "torch.manual_seed(2)\n",
    "x1_b = torch.rand(4, 4) * 3 - 1\n",
    "# x2 = torch.rand(4, 4)\n",
    "t = F.clamp(x1_b).detach()\n",
    "\n",
    "x1 = torch.rand(4, 4, requires_grad=True)\n",
    "optim_op(F.clamp, x1, t, n=2000, g=True, clip=0.1)\n",
    "print(torch.sqrt((x1 - x1_b).pow(2).mean()).item())\n",
    "print(F.clamp(x1)) \n",
    "print(F.clamp(x1_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6533, 0.7580, 0.3844, 0.3148], grad_fn=<ClampGBackward>)\n",
      "tensor([0.6533, 0.7580, 0.3844, 0.3148])\n"
     ]
    }
   ],
   "source": [
    "# Test bounded union on\n",
    "torch.manual_seed(1)\n",
    "x1_b = torch.rand(4, 4) ** 4\n",
    "# x2 = torch.rand(4, 4)\n",
    "t = F.bounded_union_on(x1_b, -1, False).detach()\n",
    "\n",
    "x1 = torch.rand(4, 4, requires_grad=True)\n",
    "optim_op(F.bounded_union_on, x1, t, dim=-1, keepdim=False, g=True, clip=0.1)\n",
    "torch.sqrt((x1 - x1_b).pow(2).mean()).item()\n",
    "print(F.bounded_union_on(x1, dim=-1, keepdim=False)) \n",
    "print(F.bounded_union_on(x1_b, dim=-1, keepdim=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3826, 0.0852, 0.4276, 0.2986], grad_fn=<ReluBackward0>)\n",
      "tensor([0.3826, 0.0852, 0.4276, 0.2986])\n"
     ]
    }
   ],
   "source": [
    "# Test bounded inter on\n",
    "torch.manual_seed(1)\n",
    "x1_b = torch.rand(4, 4) ** 0.25\n",
    "# x2 = torch.rand(4, 4)\n",
    "t = F.bounded_inter_on(x1_b, -1, False).detach()\n",
    "\n",
    "x1 = torch.rand(4, 4, requires_grad=True)\n",
    "optim_op(F.bounded_inter_on, x1, t, dim=-1, keepdim=False, g=True, clip=0.1)\n",
    "torch.sqrt((x1 - x1_b).pow(2).mean()).item()\n",
    "print(F.bounded_inter_on(x1, dim=-1, keepdim=False)) \n",
    "print(F.bounded_inter_on(x1_b, dim=-1, keepdim=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9020, 0.5041, 0.9635, 0.9469],\n",
      "        [0.3138, 0.6073, 0.4411, 0.6082],\n",
      "        [0.4450, 0.5610, 0.9645, 0.8475],\n",
      "        [0.2283, 0.8186, 0.6859, 1.0000]], grad_fn=<ClampBackward1>)\n",
      "tensor([[0.9020, 0.5041, 0.9635, 0.9469],\n",
      "        [0.3138, 0.6073, 0.4411, 0.6082],\n",
      "        [0.4450, 0.5610, 0.9645, 0.8475],\n",
      "        [0.2283, 0.8186, 0.6859, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test bounded union\n",
    "torch.manual_seed(1)\n",
    "x1_b = torch.rand(4, 4) ** 4\n",
    "x2 = torch.rand(4, 4)\n",
    "t = F.bounded_union(x1_b, x2).detach()\n",
    "\n",
    "x1 = torch.rand(4, 4, requires_grad=True)\n",
    "optim_op(F.bounded_union, x1, t, x2=x2, g=True)\n",
    "torch.sqrt((x1 - x1_b).pow(2).mean()).item()\n",
    "print(F.bounded_union(x1, x2)) \n",
    "print(F.bounded_union(x1_b, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6270, 0.2342, 0.6029, 0.6668],\n",
      "        [0.0000, 0.3393, 0.2753, 0.4018],\n",
      "        [0.3376, 0.3862, 0.6925, 0.6028],\n",
      "        [0.0000, 0.4524, 0.4806, 0.6616]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.6270, 0.2342, 0.6029, 0.6668],\n",
      "        [0.0000, 0.3393, 0.2753, 0.4018],\n",
      "        [0.3376, 0.3862, 0.6925, 0.6028],\n",
      "        [0.0000, 0.4524, 0.4806, 0.6616]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test bounded inter\n",
    "torch.manual_seed(1)\n",
    "x1_b = torch.rand(4, 4) ** 0.5\n",
    "x2 = torch.rand(4, 4) ** 0.5\n",
    "t = F.bounded_inter(x1_b, x2).detach()\n",
    "\n",
    "x1 = torch.rand(4, 4, requires_grad=True)\n",
    "optim_op(F.bounded_inter, x1, t, x2=x2, g=True)\n",
    "torch.sqrt((x1 - x1_b).pow(2).mean()).item()\n",
    "print(F.bounded_inter(x1, x2)) \n",
    "print(F.bounded_inter(x1_b, x2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
