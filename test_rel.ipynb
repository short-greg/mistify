{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(3)\n",
    "batch_size = 10000\n",
    "x = torch.rand(batch_size, 8, 1)\n",
    "w_base = torch.rand(1, 8, 16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     chosen \u001b[38;5;241m=\u001b[39m (y \u001b[38;5;241m==\u001b[39m inner)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y, chosen\n\u001b[0;32m----> 9\u001b[0m y_base, chosen_base \u001b[38;5;241m=\u001b[39m calc_chosen(\u001b[43mx\u001b[49m, w_base)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def calc_chosen(x, w):\n",
    "\n",
    "    inner = torch.min(x, w)\n",
    "    y = torch.max(inner, keepdim=True, dim=-2)[0]\n",
    "    chosen = (y == inner)\n",
    "    return y, chosen\n",
    "\n",
    "\n",
    "y_base, chosen_base = calc_chosen(x, w_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_rel(x, t, chosen=None, dim=0):\n",
    "\n",
    "    negatives = torch.relu(x - t)\n",
    "    positives = torch.min(x, t)\n",
    "    if chosen is not None:\n",
    "        positives = chosen * positives\n",
    "\n",
    "    return (\n",
    "        positives.sum(dim=dim, keepdim=True) / \n",
    "        (positives.sum(dim=dim, keepdim=True) + negatives.sum(dim=dim, keepdim=True))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'update_rel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m chosen_rel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     w_rel \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_rel\u001b[49m(x, y_base, chosen_rel)\n\u001b[1;32m      6\u001b[0m     y_rel, chosen_rel \u001b[38;5;241m=\u001b[39m calc_chosen(x, w_rel)\n\u001b[1;32m      8\u001b[0m     true_count \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      9\u001b[0m         (chosen_rel \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m&\u001b[39m (chosen_base \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m     )\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'update_rel' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# \n",
    "\n",
    "chosen_rel = None\n",
    "for i in range(5):\n",
    "    w_rel = update_rel(x, y_base, chosen_rel)\n",
    "    y_rel, chosen_rel = calc_chosen(x, w_rel)\n",
    "\n",
    "    true_count = (\n",
    "        (chosen_rel == True) & (chosen_base == True)\n",
    "    ).float()\n",
    "    fp_count = (\n",
    "        (chosen_rel == False) & (chosen_base == True)\n",
    "    ).float()\n",
    "    print(\n",
    "        'Rate: ',\n",
    "        true_count.sum() /\n",
    "        (fp_count + true_count).sum()\n",
    "    )\n",
    "    print(\n",
    "        'Loss: ', (y_rel - y_base).abs().mean()\n",
    "    )\n",
    "# for the first round i \n",
    "\n",
    "# increases to roughly 80% accuracy on the second iteration\n",
    "# indicates it should help to do an extra iteration\n",
    "\n",
    "# on the first round do not use chosen\n",
    "\n",
    "# inner_rel = torch.min(x, w_rel)\n",
    "# y_rel = inner_rel.max(dim=-2, keepdim=True)\n",
    "# chosen = (y_rel == inner_rel)\n",
    "\n",
    "# how many of the indices match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it is worth it to use rel..\n",
    "\n",
    "1. Don't know how much it will help in a realistic case but it looks quite promising here\n",
    "2. It's possible the straight through estimator is also doing as good as it can for multiple layers and that I need to add noise\n",
    "3. When there is not a clear optimum. Maybe it does not matter too much\n",
    "4. The loss decreases drastically on the second iteration. Regular gradient descent might get me the rest of the way\n",
    "5. Not sure how things will be affected by using both x_rel and w_rel. But doing just one seems to help dramatically\n",
    "\n",
    "\n",
    "For now... Implement a simpler version of the \"second order version\"\n",
    "\n",
    "\n",
    "a. Use rel loss 2\n",
    "b. gradually anneal it\n",
    "c. get rid of it\n",
    "\n",
    "d. How to deal with the regular intersection / Union?\n",
    " a. Use STE (?)\n",
    " b. Do not use it \n",
    " c. Use the \"soft version\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.46728779318966446\n",
      "1: 0.15302578010891058\n",
      "2: 0.05956998297685309\n",
      "3: 0.01666161468653362\n",
      "4: 0.003657123434233562\n",
      "5: 0.0009255634835933017\n",
      "6: 0.0009300483079756834\n",
      "7: 0.0009261109340122512\n",
      "8: 0.0009261600318058288\n",
      "9: 0.0009284274058734786\n",
      "10: 0.0009253638283691451\n",
      "11: 0.0009259603670031964\n",
      "12: 0.0009269576034909468\n",
      "13: 0.0009253395388280101\n",
      "14: 0.0009266634249199135\n",
      "15: 0.0009253652010384146\n",
      "16: 0.0009253517300173451\n",
      "17: 0.0009307490339019348\n",
      "18: 0.0009268962327225865\n",
      "19: 0.0009296386704567961\n",
      "20: 0.0009268863794181637\n",
      "21: 0.0009254572030357262\n",
      "22: 0.0009271576064874571\n",
      "23: 0.0009267109260552481\n",
      "24: 0.0009263167393547071\n",
      "25: 0.0009261602226385399\n",
      "26: 0.0009252286878638441\n",
      "27: 0.0009292198773886112\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# reg2 = 1e-6 * (1 - neuron2.weight).abs().mean()\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# reg1 = 1e-6 * (neuron1.weight).abs().mean()\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# reg2 = 1e-6 * (1 - neuron2.min_max.weight).abs().mean()\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# reg1 = 1e-6 * (neuron1.max_min.weight).abs().mean()\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# (loss + reg2 + reg2).backward()\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     85\u001b[0m zenkai\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mapply_p(\n\u001b[1;32m     86\u001b[0m     neuron1, \u001b[38;5;28;01mlambda\u001b[39;00m p: torch\u001b[38;5;241m.\u001b[39mclamp(p, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m) \n\u001b[1;32m     87\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py310torch/lib/python3.10/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py310torch/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py310torch/lib/python3.10/site-packages/torch/autograd/function.py:253\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    252\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py310torch/lib/python3.10/site-packages/zenkai/kaku/_lm2.py:497\u001b[0m, in \u001b[0;36mLearningF.backward\u001b[0;34m(ctx, *grad_outputs)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep(x, t, state, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlmode \u001b[38;5;241m==\u001b[39m LMode\u001b[38;5;241m.\u001b[39mDefault:\n\u001b[0;32m--> 497\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccumulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m     x_prime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_x(x, t, state, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlmode \u001b[38;5;241m==\u001b[39m LMode\u001b[38;5;241m.\u001b[39mStepPriority:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py310torch/lib/python3.10/site-packages/zenkai/kaku/_lm2.py:359\u001b[0m, in \u001b[0;36mStepTheta._accumulate_hook_runner\u001b[0;34m(self, x, t, state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_accumulate_hook_runner\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: IO, t: IO, state: State, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    353\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call step wrapped with the hooks\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m        x (IO): the incoming IO\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03m        t (IO): The target IO\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_accumulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m posthook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulate_hooks:\n\u001b[1;32m    362\u001b[0m         posthook(\u001b[38;5;28mself\u001b[39m, x, t, state)\n",
      "File \u001b[0;32m~/Development/mistify/rel.py:534\u001b[0m, in \u001b[0;36mMinMaxTPLearner.accumulate\u001b[0;34m(self, x, t, state, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m+\u001b[39m rec_loss\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# print(x_prime[0], x.f[0])\u001b[39;00m\n\u001b[0;32m--> 534\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py310torch/lib/python3.10/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py310torch/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from rel import (\n",
    "    train, MinMax, MinMaxTPLearner, \n",
    "    MinMaxLoss, MinMaxLearner, TruncatedGaussianNoise, \n",
    "    MaxMinLearner, MaxMin, RandNoise, SmoothSTEMaxMin, SmoothSTEMinMax\n",
    ")\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import zenkai\n",
    "from itertools import chain\n",
    "\n",
    "torch.manual_seed(3)\n",
    "batch_size = 10000\n",
    "x = torch.rand(batch_size, 32)\n",
    "\n",
    "x_weight = 1.0\n",
    "w_weight = 0.01\n",
    "n_epochs = 200\n",
    "\n",
    "neuron_base = MinMax(\n",
    "    32, 64\n",
    ")\n",
    "neuron_base2 = MinMax(\n",
    "    64, 64\n",
    ")\n",
    "\n",
    "# neuron1 = SmoothSTEMinMax(\n",
    "#     32, 64, a=10, adjust=False\n",
    "# )\n",
    "# neuron2 = SmoothSTEMinMax(\n",
    "#     64, 64, a=10, adjust=False\n",
    "# )\n",
    "neuron1 = MinMaxTPLearner(\n",
    "    32, 64, reduction='sum', rel_reduction='mean',\n",
    "    x_weight=0.01, w_weight=0.01 #, a=5\n",
    ")\n",
    "neuron2 = MinMaxTPLearner(\n",
    "    64, 64, reduction='sum', rel_reduction='mean', \n",
    "    x_weight=0.01, w_weight=0.01 #, a=5\n",
    ")\n",
    "\n",
    "# neuron1 = SmoothSTEMaxMin(\n",
    "#     32, 64, adjust=False, a=4\n",
    "# )\n",
    "# neuron2 = SmoothSTEMinMax(\n",
    "#     64, 64, adjust=False, a=4\n",
    "# )\n",
    "\n",
    "noiser = nn.Dropout(0.2)\n",
    "with torch.no_grad():\n",
    "    t = neuron_base2(neuron_base(x))\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(\n",
    "    x, t\n",
    ")\n",
    "optim = torch.optim.Adam(chain(neuron1.parameters(), neuron2.parameters()), lr=1e-2)\n",
    "\n",
    "j = 0\n",
    "for i in range(n_epochs):\n",
    "    results = []\n",
    "    for x_i, t_i in torch.utils.data.DataLoader(\n",
    "        dataset, 128, True\n",
    "    ):\n",
    "        y_i = neuron1(x_i)\n",
    "        # y_i = torch.clamp(\n",
    "        #     y_i + torch.randn_like(y_i) * 0.01, 0, 1\n",
    "        # )\n",
    "        y_i = neuron2(y_i)\n",
    "        # loss = min_max_loss(x_i, y_i, t_i)\n",
    "        loss = (y_i - t_i).pow(2).mean()\n",
    "        # print(\n",
    "        #     loss.item(),\n",
    "        #     (y_i - t_i).pow(2).sum() / y_i.numel()\n",
    "        # )\n",
    "        optim.zero_grad()\n",
    "        # reg2 = 1e-6 * (1 - neuron2.weight).abs().mean()\n",
    "        # reg1 = 1e-6 * (neuron1.weight).abs().mean()\n",
    "        \n",
    "        # reg2 = 1e-6 * (1 - neuron2.min_max.weight).abs().mean()\n",
    "        # reg1 = 1e-6 * (neuron1.max_min.weight).abs().mean()\n",
    "        # (loss + reg2 + reg2).backward()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        zenkai.params.apply_p(\n",
    "            neuron1, lambda p: torch.clamp(p, 0.0, 1.0) \n",
    "        )\n",
    "        zenkai.params.apply_p(\n",
    "            neuron2, lambda p: torch.clamp(p, 0.0, 1.0) \n",
    "        )\n",
    "        results.append(loss.item())\n",
    "        # results.append(loss.item() / y_i.numel())\n",
    "    \n",
    "    # if i % 2 == 0:\n",
    "    j += 1\n",
    "    # if (j >= 150):\n",
    "    #     neuron1.a = None\n",
    "    #     neuron2.a = None\n",
    "    # elif (j % 10) == 0:\n",
    "    #     # neuron1.w_weight = 0.0 # neuron1.w_weight * 0.5\n",
    "    #     # neuron2.w_weight = 0.0 # neuron2.w_weight * 0.5\n",
    "    #     neuron1.a = neuron1.a + 2\n",
    "    #     neuron2.a = neuron2.a + 2\n",
    "    \n",
    "    # else:\n",
    "    #     neuron1.w_weight = 0.0 # neuron1.w_weight * 0.5\n",
    "    #     neuron2.w_weight = 0.01 # neuron2.w_weight * 0.5\n",
    "\n",
    "        # j = 0\n",
    "    \n",
    "    # neuron1.w_weight = neuron1.w_weight * 0.95\n",
    "    # neuron2.w_weight = neuron2.w_weight * 0.95\n",
    "    # neuron2.x_weight = neuron2.x_weight * 0.95\n",
    "    # neuron1.a = neuron1.a + 1\n",
    "    #neuron2.a = neuron2.a + 1\n",
    "    # else:\n",
    "    #     neuron1.w_weight = neuron1.w_weight * 0.95\n",
    "    #     neuron2.w_weight = 0.0\n",
    "    #     neuron2.x_weight = 0.1\n",
    "\n",
    "    print(f'{i}: {np.mean(results)}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
